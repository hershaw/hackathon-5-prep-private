{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "\nNothing to see here"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# SKLearn related imports\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "# NLTK Text Processing package\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Data in Practice \n",
        "\nWhile the first learning unit looked at the different steps inherent in working with textual data from a \"reinventing the wheel perspective\", this learning unit introduces the already existing tools and packages we can use to work with text."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example we will use a dataset called UCI \"News Aggregator\". \n",
        "\n",
        "This dataset is a collection of news headlines and their corresponding category. Our objective is to apply text classification techniques to assign a category to a news headline.\n",
        "\nLets start by importing the data:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./data/uci-news-aggregator.csv')\n",
        "df.dtypes"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": [
              "ID            int64\n",
              "TITLE        object\n",
              "URL          object\n",
              "PUBLISHER    object\n",
              "CATEGORY     object\n",
              "STORY        object\n",
              "HOSTNAME     object\n",
              "TIMESTAMP     int64\n",
              "dtype: object"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 3,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see this dataset has 8 different fields, for the sake of this learning unit we will look at two of them: Title and Category."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['TITLE', 'CATEGORY']]\n",
        "df.columns = ['title', 'category']\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": [
              "                                               title category\n",
              "0  Fed official says weak data caused by weather,...        b\n",
              "1  Fed's Charles Plosser sees high bar for change...        b\n",
              "2  US open: Stocks fall after Fed official hints ...        b\n",
              "3  Fed risks falling 'behind the curve', Charles ...        b\n",
              "4  Fed's Plosser: Nasty Weather Has Curbed Job Gr...        b"
            ],
            "text/html": [
              "<div>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Fed official says weak data caused by weather,...</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 4,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(df['title'][2], df['category'][2])"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": [
              "('US open: Stocks fall after Fed official hints at accelerated tapering', 'b')"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the keywords in this small sample we can already see that the \"b\" category seems to be related to stock related news.\n",
        "\nBefore moving into vectorizing the titles into a structured format our machine learning models can understand, lets split our dataset into a training and validation set."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Split in train and validation\n",
        "train_df, validation_df = train_test_split(df, test_size=0.2, random_state=42)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of words representation\n",
        "\n",
        "The first package we will look at are the text functionalities which come with the scikit learn package that you already used in the past.\n",
        "\n",
        "As we have seen in the first learning unit, a commonly used method to vectorize a piece of text is through a so called bag of words representation.\n",
        "\n",
        "Scikit Learn comes with a handy tool for this procedure, called __[Count Vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)__ .\n",
        "\nLets instantiate a instance and see how it works."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Like every scikit transformer this module needs to be fit first, in this case, this means it needs to build an internal dictionary of the available words in the text."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.fit(train_df['title'].values)\n",
        "\n",
        "# Looking at a small sample of the vocabulary:\n",
        "vocabulary = list(vectorizer.vocabulary_.keys())\n",
        "print(\"Small sample of the vocabulary:\", vocabulary[0:20])\n",
        "\n",
        "# Number of words in the vocabulary\n",
        "print(\"\\nNumber of distinct words:\", len(vocabulary))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small sample of the vocabulary: ['nasa', 'cassini', 'spacecraft', 'finds', '101', 'geysers', 'on', 'icy', 'saturn', 'moon', 'paul', 'mazursky', 'dead', 'five', 'times', 'oscar', 'nominated', 'director', 'has', 'died']\n",
            "\n",
            "Number of distinct words: 50140\n"
          ]
        }
      ],
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at a random sample sentence, for example sentence 61, we can visualize the bag of words representation:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = train_df['title'].values[61:62]\n",
        "print(sentence[0], '\\n')\n",
        "\n",
        "# Tranform sentence into bag of words representation\n",
        "word_count_sentence = vectorizer.transform(sentence)\n",
        "\n",
        "# Find the indexes of the words which appear in the sentence\n",
        "_, columns = word_count_sentence.nonzero()\n",
        "\n",
        "# Get the inverse map to map vector indexes to words\n",
        "vocabulary = vectorizer.vocabulary_\n",
        "inv_map = {v: k for k, v in vocabulary.items()}\n",
        "\n",
        "# Extract the corresponding word and count\n",
        "counts = [(inv_map[i], word_count_sentence[0, i]) for i in columns]\n",
        "\n",
        "for word, count in counts:\n",
        "    print(word, \": \", count)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NASA starts testing 'flying saucer', landing men on Mars \n",
            "\n",
            "flying :  1\n",
            "landing :  1\n",
            "mars :  1\n",
            "men :  1\n",
            "nasa :  1\n",
            "on :  1\n",
            "saucer :  1\n",
            "starts :  1\n",
            "testing :  1\n"
          ]
        }
      ],
      "execution_count": 9,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now get the word counts (bag of word representation) for every sentence by calling the transform method. This returns a sparse matrix (which you already used in the last hackathon) where the rows represent the samples and the columns the word counts."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "word_count_matrix = vectorizer.transform(train_df['title'].values)\n",
        "word_count_matrix.shape\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": [
              "(337935, 50140)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF (Term Frequencyâ€“Inverse Document Frequency)\n",
        "\n",
        "The next important step is to scale the feature vectors by the terms frequencies so they don't skew the results.\n",
        "\nScikit comes with a handy tool called __[TF-IDF Transformer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)__ which deals with this for us."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfTransformer()\n",
        "tfidf.fit(word_count_matrix)\n",
        "\nword_term_frequency_matrix = tfidf.transform(word_count_matrix)"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming\n",
        "\n",
        "While this gives us a good initial representation it can be further improved!\n",
        "\n",
        "An example improvement, which was already introduced in the first lesson is stemming.\n",
        "\n",
        "Stemming is implemented in the __[NLTK](http://www.nltk.org/)__ (Natural Language Toolkit) Python package which comes with a lot of usefull tools like entity recognition and parsing for text processing!\n",
        "\nHere we will encapsulate the stemming module as a custom Scikit transformer. This transformer separates the sentences into words, stems the words and joins the sentence back together."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom transformer to implement stemming and sentence cleaning\n",
        "\n",
        "class StemmerTransformer(TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "        self.tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        \n",
        "    def transform(self, X, *_):\n",
        "        X = list(map(self._clean_sentence, X))\n",
        "        return X\n",
        "    \n",
        "    def _clean_sentence(self, sentence):\n",
        "        # Split sentence into list of words\n",
        "        words = self.tokenizer.tokenize(sentence)\n",
        "        \n",
        "        # Filter out stopwords\n",
        "        #words = [word for word in words if word not in stopwords.words('english')]\n",
        "        \n",
        "        # Filter out numbers\n",
        "        words = [x for x in words if not x.isdigit()]\n",
        "        \n",
        "        # Stem words\n",
        "        words = map(self.stemmer.stem, words)\n",
        "        \n",
        "        # Join list elements into string\n",
        "        sentence = \" \".join(words)\n",
        "        \n",
        "        return sentence\n",
        "    \n",
        "    def fit(self, *_):\n",
        "        return self"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = StemmerTransformer()\n",
        "\n",
        "original_sentence = train_df['title'].values[25:26]\n",
        "stemmed_sentence = stemmer.transform(original_sentence)\n",
        "\n",
        "print(\"Original sentence:\\n\", original_sentence, \"\\n\\nStemmed sentence:\\n\", stemmed_sentence)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original sentence:\n",
            " ['Game of Thrones Gives Us the Best Wedding Gift Imaginable'] \n",
            "\n",
            "Stemmed sentence:\n",
            " ['game of throne give us the best wed gift imagin']\n"
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipelines\n",
        "\n",
        "To make the process from original text snippet to final feature vector as flexible and clean as possible we will use the SciKit __[Pipeline API](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)__. Pipelines allow us to easily compose transformations and classifiers.\n",
        "\nThe main advantage of pipelines is that the pipeline exposes the fit and predict functions, these automatically call the transformations on the data and the classifier, keeping the transformations coherent between train and test data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the pipeline\n",
        "text_clf = Pipeline([('stemm', StemmerTransformer()),\n",
        "                   ('vect', CountVectorizer()),\n",
        "                   ('tfidf', TfidfTransformer()),\n",
        "                   ('clf', MultinomialNB())])"
      ],
      "outputs": [],
      "execution_count": 25,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final piece that is missing is converting the character labels into numeric labels through the Scikit __[Label Encoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)__ tool."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the labels\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(train_df['category'].values)\n",
        "\n",
        "train_df['category'] = le.transform(train_df['category'].values)\n",
        "validation_df['category'] = le.transform(validation_df['category'].values)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "C:\\Users\\Paul\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "C:\\Users\\Paul\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
          ]
        }
      ],
      "execution_count": 26,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the classifier\n",
        "\ntext_clf.fit(map(str, train_df['title'].values), train_df['category'].values)  "
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted = text_clf.predict(map(str, validation_df['title'].values))\n",
        "np.mean(predicted == validation_df['category'])\n"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "CountVectorizer - Vocabulary wasn't fitted.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-91048b38bb3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mvalidation_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mC:\\Users\\Paul\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mC:\\Users\\Paul\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mC:\\Users\\Paul\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mC:\\Users\\Paul\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mC:\\Users\\Paul\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[1;31m# FIXME NotFittedError_ --> NotFittedError in 0.19\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[1;32mraise\u001b[0m \u001b[0m_NotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: CountVectorizer - Vocabulary wasn't fitted."
          ]
        }
      ],
      "execution_count": 27,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "0.3.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}